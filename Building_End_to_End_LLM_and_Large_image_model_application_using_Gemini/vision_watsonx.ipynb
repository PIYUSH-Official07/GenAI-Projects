{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pwd\n",
    "import os\n",
    "from collections.abc import Iterable\n",
    "from typing import Any, Optional\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from constant import api_key, watsonx_project_id\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FLAN_T5_XXL', 'FLAN_UL2', 'MT0_XXL', 'GPT_NEOX', 'MPT_7B_INSTRUCT2', 'STARCODER', 'LLAMA_2_70B_CHAT', 'LLAMA_2_13B_CHAT', 'GRANITE_13B_INSTRUCT', 'GRANITE_13B_CHAT', 'FLAN_T5_XL', 'GRANITE_13B_CHAT_V2', 'GRANITE_13B_INSTRUCT_V2', 'ELYZA_JAPANESE_LLAMA_2_7B_INSTRUCT', 'MIXTRAL_8X7B_INSTRUCT_V01_Q', 'CODELLAMA_34B_INSTRUCT_HF', 'GRANITE_20B_MULTILINGUAL']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from langchain.llms import HuggingFaceHub\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "print([model.name for model in ModelTypes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "model_id = ModelTypes.LLAMA_2_70B_CHAT\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.5,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "\n",
    "llama = Model(\n",
    "    model_id=model_id, \n",
    "    params=parameters, \n",
    "    credentials=credentials,\n",
    "    project_id=watsonx_project_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "llm = WatsonxLLM(model=llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='.\\n\\n<a href=\"https://i.stack.imgur.com/Mu8K3.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\\n\\nThe image shows a flowchart for a system that takes in user input and processes it to generate a response. Here\\'s a breakdown of the flowchart:\\n\\n1. Start: The system starts when a user inputs a query or request.\\n2. Natural Language Processing (NLP): The input'\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# Function to compress the image\n",
    "def compress_image(input_file, output_file, quality=15):\n",
    "    with Image.open(input_file) as img:\n",
    "        img.save(output_file, format='JPEG', quality=quality)\n",
    "\n",
    "# Compress the image\n",
    "compressed_file = 'compressed_tstllm.jpg'\n",
    "compress_image('tstllm.jpg', compressed_file)\n",
    "input=\"please explain the given image\"\n",
    "\n",
    "# Read and encode the compressed image\n",
    "with open(compressed_file, 'rb') as image_file:\n",
    "    image_data = image_file.read()\n",
    "\n",
    "# Encode the binary data to Base64\n",
    "img = base64.b64encode(image_data).decode('utf-8')\n",
    "\n",
    "# Ensure the data size is within the limit\n",
    "max_tokens = 4096\n",
    "if len(img) > max_tokens:\n",
    "    print(\"Image data exceeds the token limit. Please further compress or reduce the image size.\")\n",
    "else:\n",
    "    response = llm.generate([input, img])\n",
    "    print(response.generations[0][0])\n",
    "\n",
    "# Cleanup the compressed file\n",
    "#os.remove(compressed_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Path to the image file\n",
    "uploaded_file = 'compressed_tstllm.jpg'\n",
    "\n",
    "# Read the image file in binary mode\n",
    "with open(uploaded_file, 'rb') as image_file:\n",
    "    # Read the contents of the file\n",
    "    image_data = image_file.read()\n",
    "\n",
    "# Encode the binary data to Base64\n",
    "img= base64.b64encode(image_data).decode('utf-8')\n",
    "input=\"please explain the given image\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.generate([input,img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='.\\n\\n<a href=\"https://i.stack.imgur.com/KLlY1.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\\n\\nThe image shows a flowchart for a system that allows users to order and pay for food online. The flowchart illustrates the various steps involved in the process, including the user\\'s actions and the system\\'s responses.\\n\\nHere\\'s a breakdown of the flowchart:\\n\\n1. User')], [Generation(text='/A..,\"(j,/a\"Tk/\\n..0k 0%(4.0?).1.0. 7.7p.0k\\n.0.3l(1\\n.1 (2llNNBg,.NoW 4\\n,NIrD (\\n;W(, 0 ./rl 5 (6 2 I\".T,\\n.R?,')]], llm_output=None, run=[RunInfo(run_id=UUID('57cc3d0c-8432-49e0-a09f-a0970423d5c8')), RunInfo(run_id=UUID('ee63c6f9-e189-4cfd-970f-66c8e988c328'))], type='LLMResult')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\n\\n<a href=\"https://i.stack.imgur.com/KLlY1.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\\n\\nThe image shows a flowchart for a system that allows users to order and pay for food online. The flowchart illustrates the various steps involved in the process, including the user\\'s actions and the system\\'s responses.\\n\\nHere\\'s a breakdown of the flowchart:\\n\\n1. User'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
